{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp k_fold_validation\n",
    "\n",
    "# export\n",
    "import sys\n",
    "from random import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import logging\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from job_offer_classifier.pipeline_classifier import Pipeline\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "#logging config\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Fold Validation \n",
    "> Aggregates the K fold validation to the pipeline classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the performance of the model the, sklearn K fold validation  method is incorporated. After running the *k-fold* method the averaged scores are computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn K Fold Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def k_fold_validation(X,n_splits=4):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    kf.get_n_splits(X)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        yield X.iloc[train_index], X.iloc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the train and test set splitting of of equal size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(train.shape[0] == test.shape[0]  for train,test in k_fold_validation(pd.DataFrame(range(100)),n_splits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average over scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def score_averages(**k_fold_evaluations):\n",
    "    fold1 = next(iter(k_fold_evaluations.values()))\n",
    "    split_names = fold1.keys()\n",
    "    score_names = next(iter(fold1.values())).keys()\n",
    "    K = len(k_fold_evaluations)\n",
    "    return {\n",
    "        split: {\n",
    "            score_name: sum(\n",
    "                k_fold_evaluations[k][split][score_name]\n",
    "                for k in k_fold_evaluations.keys()\n",
    "            ) / K\n",
    "            for score_name in score_names\n",
    "        }\n",
    "        for split in split_names\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case of two folds in a train set with 'acc' and 'f1' scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd1, rnd2, rnd3, rnd4 = [random() for _ in range(4)]\n",
    "fold1 = {'train': {'acc': rnd1, 'f1': rnd2}}\n",
    "fold2 = {'train': {'acc': rnd3, 'f1': rnd4}}\n",
    "avg_acc, avg_f1 = (rnd1 + rnd3) / 2, (rnd2 + rnd4) / 2\n",
    "\n",
    "train_score_avgs = {'train': {'acc': avg_acc, 'f1': avg_f1}}\n",
    "\n",
    "assert score_averages(fold1=fold1, fold2=fold2) == train_score_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class KFoldPipeline(Pipeline):\n",
    "    '''K fold validation over the model built in `Pipeline` class\n",
    "    '''\n",
    "    def __init__(self, dataset_file, n_splits=4):\n",
    "        self.n_splits = n_splits\n",
    "        self.k_fold_evaluations = {}\n",
    "        Pipeline.__init__(self, src_file=dataset_file)\n",
    "\n",
    "    def k_fold_validation(self):\n",
    "        ''' Implements the `pipeline` method for each fold.\n",
    "            The averaged score is stored in `avg_evaluation`\n",
    "        '''\n",
    "        for k, split in enumerate(\n",
    "            k_fold_validation(self.data, n_splits=self.n_splits)\n",
    "        ):\n",
    "            self.dfs = {'train': split[0], 'test': split[1]}\n",
    "            self.pipeline()\n",
    "            self.k_fold_evaluations[str(k + 1)] = self.evaluation\n",
    "\n",
    "            logging.info(f'fold {k+1} has finished...')\n",
    "            for key in ('accuracy','f1_score'):\n",
    "                logging.info(\n",
    "                    'The %s score for the training set in fold %s is %s' %\n",
    "                    (key,k+1,self.evaluation['train'][key])\n",
    "                )\n",
    "\n",
    "        self.avg_evaluation = score_averages(**self.k_fold_evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"KFoldPipeline.k_fold_validation\" class=\"doc_header\"><code>KFoldPipeline.k_fold_validation</code><a href=\"__main__.py#L10\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>KFoldPipeline.k_fold_validation</code>()\n",
       "\n",
       "Implements the `pipeline` method for each fold. \n",
       "The averaged score is stored in `avg_evaluation`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nbdev.showdoc import *\n",
    "show_doc(KFoldPipeline.k_fold_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The case $k=1$*  \n",
    "This case is used to check the pipeline (through the info logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:58:14 INFO: Using /tmp/tfhub_modules to cache modules.\n",
      "08:58:26 INFO: fold 1 has finished...\n",
      "08:58:26 INFO: The accuracy score for the training set in fold 1 is 1.0\n",
      "08:58:26 INFO: The f1_score score for the training set in fold 1 is 1.0\n",
      "08:58:38 INFO: fold 2 has finished...\n",
      "08:58:38 INFO: The accuracy score for the training set in fold 2 is 0.8296703\n",
      "08:58:38 INFO: The f1_score score for the training set in fold 2 is 0.8724279570683305\n"
     ]
    }
   ],
   "source": [
    "kfp = KFoldPipeline(dataset_file='../data/interim/payloads.csv',n_splits=2)\n",
    "kfp.train_steps = 100\n",
    "kfp.k_fold_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The case $k=4$*  \n",
    "This case represents the actual assesment of the model perfomance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:17:30 INFO: fold 1 has finished...\n",
      "09:17:30 INFO: The accuracy score for the training set in fold 1 is 0.99655175\n",
      "09:17:30 INFO: The f1_score score for the training set in fold 1 is 0.9979633420684656\n",
      "09:18:06 INFO: fold 2 has finished...\n",
      "09:18:06 INFO: The accuracy score for the training set in fold 2 is 0.98275864\n",
      "09:18:06 INFO: The f1_score score for the training set in fold 2 is 0.9905482064335733\n",
      "09:18:43 INFO: fold 3 has finished...\n",
      "09:18:43 INFO: The accuracy score for the training set in fold 3 is 0.9862069\n",
      "09:18:43 INFO: The f1_score score for the training set in fold 3 is 0.9908257247813479\n",
      "09:19:24 INFO: fold 4 has finished...\n",
      "09:19:24 INFO: The accuracy score for the training set in fold 4 is 0.9862543\n",
      "09:19:24 INFO: The f1_score score for the training set in fold 4 is 0.9908675778009323\n",
      "09:20:03 INFO: fold 5 has finished...\n",
      "09:20:03 INFO: The accuracy score for the training set in fold 5 is 0.9862543\n",
      "09:20:03 INFO: The f1_score score for the training set in fold 5 is 0.9908675778009323\n"
     ]
    }
   ],
   "source": [
    "kfp = KFoldPipeline(dataset_file='../data/interim/payloads.csv',n_splits=5)\n",
    "kfp.train_steps = 5000 #default\n",
    "kfp.k_fold_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  averaged evaluation is in `avg_evaluation` atrribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9876051664352417,\n",
       " 'accuracy_baseline': 0.7989667177200317,\n",
       " 'auc': 0.9924227833747864,\n",
       " 'auc_precision_recall': 0.9978546500205994,\n",
       " 'average_loss': 0.0626331850886345,\n",
       " 'label/mean': 0.7989667177200317,\n",
       " 'loss': 0.05038395039737224,\n",
       " 'precision': 0.9872474074363708,\n",
       " 'prediction/mean': 0.799962329864502,\n",
       " 'recall': 0.9972434759140014,\n",
       " 'global_step': 5000.0,\n",
       " 'f1_score': 0.9922144857770503}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfp.avg_evaluation['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9259893417358398,\n",
       " 'accuracy_baseline': 0.8465753316879272,\n",
       " 'auc': 0.36761903762817383,\n",
       " 'auc_precision_recall': 0.9573580622673035,\n",
       " 'average_loss': 0.1822646100074053,\n",
       " 'label/mean': 0.7999999940395355,\n",
       " 'loss': 0.1822646100074053,\n",
       " 'precision': 0.9399641513824463,\n",
       " 'prediction/mean': 0.7845720887184143,\n",
       " 'recall': 0.920714282989502,\n",
       " 'global_step': 5000.0,\n",
       " 'f1_score': 0.9279590007548488}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfp.avg_evaluation['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
