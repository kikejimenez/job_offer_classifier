{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from unittest.mock import Mock\n",
    "\n",
    "import logging\n",
    "from random import randint,random\n",
    "from itertools import product\n",
    "\n",
    "from job_offer_classifier.pipeline_classifier import Pipeline\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "#logging config\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Validations\n",
    "> Validations for the Sentiment Classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the performance of the model, the K fold validation is incorporated. After running the *k-fold* method, the averaged scores are computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Log Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def log_status(k, evaluation, *args):\n",
    "    '''Log current fold accuracy and F1 scores\n",
    "    '''\n",
    "    for key in ('accuracy', 'f1_score'):\n",
    "        logging.info(\n",
    "            'The %s score for the test set in fold %s is %s' %\n",
    "            (key, k, evaluation['test'][key]) + 5 * ' '\n",
    "        )\n",
    "    if args:\n",
    "        logging.info('\\n'.join(args))\n",
    "    logging.info(f'fold {k} has finished...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the custom logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:46:34 INFO: The accuracy score for the test set in fold 0 is 0.5     \n",
      "07:46:34 INFO: The f1_score score for the test set in fold 0 is 0.3333333333333333     \n",
      "07:46:34 INFO: this is extra info\n",
      "07:46:34 INFO: fold 0 has finished...\n"
     ]
    }
   ],
   "source": [
    "log_status(0,{'test':{'accuracy': 1/2.,'f1_score':1/3}},'this is extra info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average over Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def average_scores(**scores):\n",
    "    ''' For each dataset and score, calculates the average over folds \\n\n",
    "    '''\n",
    "    sc_vals = next(iter(scores.values()))\n",
    "    average = lambda sv: {\n",
    "        sc: sum(scores[k][sv][sc] for k in scores) / len(scores)\n",
    "        for sc in next(iter(sc_vals.values()))\n",
    "    }\n",
    "    return {sv: average(sv) for sv in sc_vals}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case of two folds in a train set with 'acc' and 'f1' scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd1, rnd2, rnd3, rnd4 = [random() for _ in range(4)]\n",
    "fold1 = {'train': {'acc': rnd1, 'f1': rnd2}}\n",
    "fold2 = {'train': {'acc': rnd3, 'f1': rnd4}}\n",
    "avg_acc, avg_f1 = (rnd1 + rnd3) / 2, (rnd2 + rnd4) / 2\n",
    "\n",
    "train_score_avgs = {'train': {'acc': avg_acc, 'f1': avg_f1}}\n",
    "\n",
    "assert average_scores(fold1=fold1, fold2=fold2) == train_score_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class KFoldPipe(Pipeline):\n",
    "    ''' Inherents [`Pipeline`](job_offer_classfier/pipeline_classifier) \n",
    "    and incorporates the k fold validation\n",
    "    '''\n",
    "    def __init__(self, n_splits, **args):\n",
    "        super().__init__(**args)\n",
    "        self.n_splits = n_splits\n",
    "        self.best_score = -1.0\n",
    "        self.best_seed = None\n",
    "        self.history = {}\n",
    "        self.frac = 1 - (1 / n_splits)\n",
    "        self.get_seeds()\n",
    "\n",
    "    def get_seeds(self):\n",
    "        ''' Array of random seeds\n",
    "        '''\n",
    "        self.seeds = {\n",
    "            str(k + 1): randint(0, 2**32 - 1)\n",
    "            for k in range(self.n_splits)\n",
    "        }\n",
    "\n",
    "    def update_best_score(self):\n",
    "        '''Update the folds and seeds that correspond to\n",
    "           the best score\n",
    "        '''\n",
    "        f1_score = self.evaluation['test']['f1_score']\n",
    "        if self.best_score < f1_score:\n",
    "            self.best_score = f1_score\n",
    "            self.best_fold = self.current_fold\n",
    "            self.best_seed = self.random_state\n",
    "\n",
    "    def update_history(self, fold):\n",
    "        ''' Update history records\n",
    "        '''\n",
    "        log_status(fold, self.evaluation)\n",
    "        self.history[fold] = self.evaluation\n",
    "\n",
    "    def average_evaluations(self):\n",
    "        '''Average over evaluation results\n",
    "        '''\n",
    "        self.averages = average_scores(**self.history)\n",
    "\n",
    "    def k_fold_validation(self):\n",
    "        ''' Runs the pipeline over all the seeds.\n",
    "        Stores the best seed, keeps evaluations history and averages over the scores. \\n\n",
    "        On each iteration: \\n\n",
    "           -splits the data over the seed of the current iteration \\n\n",
    "           -run the pipeline \\n\n",
    "           -update history and best scores \\n\n",
    "        \n",
    "        '''\n",
    "\n",
    "        for k, seed in self.seeds.items():\n",
    "            self.random_state = seed\n",
    "            self.current_fold = k\n",
    "            self.split_dataset()\n",
    "            self.pipeline()\n",
    "            self.update_history(fold=k)\n",
    "            self.update_best_score()\n",
    "        self.average_evaluations()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"KFoldPipe.get_seeds\" class=\"doc_header\"><code>KFoldPipe.get_seeds</code><a href=\"__main__.py#L15\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>KFoldPipe.get_seeds</code>()\n",
       "\n",
       "Array of random seeds\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"KFoldPipe.average_evaluations\" class=\"doc_header\"><code>KFoldPipe.average_evaluations</code><a href=\"__main__.py#L39\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>KFoldPipe.average_evaluations</code>()\n",
       "\n",
       "Average over evaluation results\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"KFoldPipe.update_best_score\" class=\"doc_header\"><code>KFoldPipe.update_best_score</code><a href=\"__main__.py#L23\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>KFoldPipe.update_best_score</code>()\n",
       "\n",
       "Update the folds and seeds that correspond to\n",
       "the best score"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"KFoldPipe.update_history\" class=\"doc_header\"><code>KFoldPipe.update_history</code><a href=\"__main__.py#L33\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>KFoldPipe.update_history</code>(**`fold`**)\n",
       "\n",
       "Update history records\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"KFoldPipe.k_fold_validation\" class=\"doc_header\"><code>KFoldPipe.k_fold_validation</code><a href=\"__main__.py#L44\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>KFoldPipe.k_fold_validation</code>()\n",
       "\n",
       "Runs the pipeline over all the seeds.\n",
       "Stores the best seed, keeps evaluations history and averages over the scores. \n",
       "\n",
       "On each iteration: \n",
       "\n",
       "   -splits the data over the seed of the current iteration \n",
       "\n",
       "   -run the pipeline \n",
       "\n",
       "   -update history and best scores "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nbdev.showdoc import *\n",
    "show_doc(KFoldPipe.get_seeds)\n",
    "show_doc(KFoldPipe.average_evaluations)\n",
    "show_doc(KFoldPipe.update_best_score)\n",
    "show_doc(KFoldPipe.update_history)\n",
    "show_doc(KFoldPipe.k_fold_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check functions in KFoldPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:46:35 INFO: The accuracy score for the test set in fold 1 is 0.55     \n",
      "07:46:35 INFO: The f1_score score for the test set in fold 1 is 0.5     \n",
      "07:46:35 INFO: fold 1 has finished...\n",
      "07:46:35 INFO: The accuracy score for the test set in fold 2 is 0.7     \n",
      "07:46:35 INFO: The f1_score score for the test set in fold 2 is 0.6     \n",
      "07:46:35 INFO: fold 2 has finished...\n"
     ]
    }
   ],
   "source": [
    "kfp = KFoldPipe(n_splits=4, src_file='../data/interim/payloads.csv')\n",
    "kfp.evaluation = {'test': {'f1_score': 0.5,'accuracy':0.55}}\n",
    "kfp.random_state = 45\n",
    "kfp.current_fold = 1\n",
    "kfp.update_best_score()\n",
    "kfp.update_history(fold='1')\n",
    "\n",
    "assert kfp.best_score == 0.5 and kfp.best_seed == 45\n",
    "assert kfp.history == {'1': kfp.evaluation}\n",
    "\n",
    "kfp.evaluation = {'test': {'f1_score': 0.6,'accuracy':0.7}}\n",
    "kfp.update_history(fold='2')\n",
    "kfp.average_evaluations()\n",
    "\n",
    "assert kfp.averages  == {'test': {'f1_score': (0.6 + 0.5)/2,'accuracy':(0.55+0.7)/2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:46:35 INFO: Using /tmp/tfhub_modules to cache modules.\n",
      "07:47:15 INFO: The accuracy score for the test set in fold 1 is 0.96694213     \n",
      "07:47:15 INFO: The f1_score score for the test set in fold 1 is 0.9791666439139396     \n",
      "07:47:15 INFO: fold 1 has finished...\n",
      "07:47:49 INFO: The accuracy score for the test set in fold 2 is 0.9586777     \n",
      "07:47:49 INFO: The f1_score score for the test set in fold 2 is 0.974093273148837     \n",
      "07:47:49 INFO: fold 2 has finished...\n",
      "07:48:25 INFO: The accuracy score for the test set in fold 3 is 0.94214875     \n",
      "07:48:25 INFO: The f1_score score for the test set in fold 3 is 0.9637305687064565     \n",
      "07:48:25 INFO: fold 3 has finished...\n"
     ]
    }
   ],
   "source": [
    "kfp = KFoldPipe(n_splits=3,src_file='../data/interim/payloads.csv')\n",
    "kfp.k_fold_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hightest of the F1 scores in *history* attribute coincides with *best_score*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert max(\n",
    "    kfp.history[str(k + 1)]['test']['f1_score'] for k in range(kfp.n_splits)\n",
    ") == kfp.best_score\n",
    "\n",
    "assert kfp.history[kfp.best_fold]['test']['f1_score'] == kfp.best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new instance of the KFoldPipe and run the pipeline with the best seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = KFoldPipe(n_splits=3,src_file='../data/interim/payloads.csv',random_state =kfp.best_seed)\n",
    "pl.pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation in this pipeline is equal to the evaluation corresponding to the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The evaluations are equal up to a factor of 2x10^(-1)\n"
     ]
    }
   ],
   "source": [
    "def min_factors(x, y):\n",
    "    ''' Outputs the  minimum integer values n,m that satisfy\n",
    "     *abs(x-y) < m x 10 ^ (n)*\n",
    "     and restricted to n=(-4,-3,-1) and m=(1,...,9)\n",
    "    '''\n",
    "    equal_abs = lambda x, y, m, n: abs(x - y) < m * 10**(n)\n",
    "    for n, m in product(range(-4, 0), range(1, 10)):\n",
    "        if equal_abs(x, y, m, n):\n",
    "            return n, m\n",
    "    return None\n",
    "\n",
    "\n",
    "n, m = sorted(\n",
    "    min_factors(pl.evaluation[dat][sc], kfp.history[kfp.best_fold][dat][sc])\n",
    "    for dat in pl.evaluation for sc in pl.evaluation['train']\n",
    ")[-1]\n",
    "\n",
    "print(f'The evaluations are equal up to a factor of {m}x10^({n})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(\n",
    "    abs(pl.evaluation[dat][sc] - kfp.history[kfp.best_fold][dat][sc]) < m *\n",
    "    10**(n) for dat in pl.evaluation for sc in pl.evaluation['train']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9559228618939718,\n",
       " 'accuracy_baseline': 0.8016529083251953,\n",
       " 'auc': 0.9919100801150004,\n",
       " 'auc_precision_recall': 0.9979395270347595,\n",
       " 'average_loss': 0.10010440647602081,\n",
       " 'label/mean': 0.8016529083251953,\n",
       " 'loss': 0.10010440647602081,\n",
       " 'precision': 0.9791301290194193,\n",
       " 'prediction/mean': 0.7820513447125753,\n",
       " 'recall': 0.9656357367833456,\n",
       " 'global_step': 5000.0,\n",
       " 'f1_score': 0.9723301619230776}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfp.averages['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_merge_datasets.ipynb.\n",
      "Converted 01_pipeline_classifier.ipynb.\n",
      "Converted 02_validations.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
