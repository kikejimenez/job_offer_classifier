{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pipeline_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8c73446dc307>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtempfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemporaryDirectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# export\n",
    "import shutil\n",
    "from random import randint\n",
    "from os import path,listdir\n",
    "from shutil import rmtree\n",
    "from tempfile import TemporaryDirectory\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline \n",
    "> Pipeline of the text binary classifier for job offer email responses "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The emails are classified as 'positive' or 'negative' using the Tensorflow `tf.estimator` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Shut down logging except Error\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset of email responses is loaded from a csv file.  Three steps make up the data processing pipeline:  \n",
    "Load the data -->  Make train-test split  -->  Create the Tensorflow input functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and split the data using the Pandas library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def split_dataframe(df, **kwargs):\n",
    "    '''Split with the pandas `sample` method.\n",
    "    '''\n",
    "    train = df.sample(**kwargs)\n",
    "    test = df.drop(train.index)\n",
    "    return {'train': train, 'test': test}\n",
    "\n",
    "\n",
    "\n",
    "def balanced_labels_in_split(df, **kwargs):\n",
    "    '''Give a balanced train-test split in 'positive' and 'negative' labels\n",
    "    '''\n",
    "    positives = split_dataframe(df[df.sentiment == 1], **kwargs)\n",
    "    negatives = split_dataframe(df[df.sentiment == 0], **kwargs)\n",
    "    train = pd.concat([positives['train'], negatives['train']])\n",
    "    test = pd.concat([positives['test'], negatives['test']])\n",
    "\n",
    "    return {'train': train, 'test': test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test these function with the following example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = lambda sentiment, n: {'sentiment': [sentiment for _ in range(n)]}\n",
    "df_example_f = lambda sentiment, ni, nf: pd.DataFrame(\n",
    "    sentiments(sentiment, nf - ni), index=range(ni, nf)\n",
    ").sample(nf - ni)\n",
    "df_a =  df_example_f(1,0,100) # positive\n",
    "df_b = df_example_f(0,150,200) # negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = randint(0, 2**32)\n",
    "split_aa = split_dataframe(df_a, random_state=random_state, frac=.55)\n",
    "split_bb = split_dataframe(df_a, random_state=random_state, frac=.55)\n",
    "\n",
    "assert all(\n",
    "    np.all((split_aa[k] == split_bb[k]).values) for k in ('train', 'test')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_dataframe(df_a, frac=0.7).values()\n",
    "assert train.size == 70 and test.size == 30\n",
    "\n",
    "dataset = pd.concat([df_a, df_b]).sample(frac=1.0)\n",
    "train, test = balanced_labels_in_split(dataset, frac=.8).values()\n",
    "\n",
    "assert dataset.size == train.size + test.size\n",
    "assert train.size / dataset.size\n",
    "\n",
    "random_state = randint(0, 2**32)\n",
    "balanced1 = balanced_labels_in_split(dataset, random_state=random_state, frac=.65)\n",
    "balanced2 = balanced_labels_in_split(dataset, random_state=random_state, frac=.65)\n",
    "assert all(\n",
    "    np.all((balanced1[k] == balanced2[k]).values) for k in ('train', 'test')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tensorflow input functions are the input elements of the Tensorflow algorithms.\n",
    "These functions are created with the help of the *version 1* `tf.estimator,inputs` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def tf_input_fn(df, **kwargs):\n",
    "    ''' Load a TF function for a DataFrame\n",
    "    '''\n",
    "    return tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
    "        df, df[\"sentiment\"],  **kwargs\n",
    "    )\n",
    "\n",
    "def train_input_fs(**kwargs):\n",
    "    '''TF functions with parameters for training\n",
    "    '''\n",
    "    return {\n",
    "        name: tf_input_fn(df, shuffle=True, num_epochs=None)\n",
    "        for name, df in kwargs.items()\n",
    "    }\n",
    "\n",
    "def predict_input_fs(**kwargs):\n",
    "    '''TF with with parameters for testing\n",
    "    '''\n",
    "    return {\n",
    "        name: tf_input_fn(df,shuffle=False)  for name, df in kwargs.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load and split the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/interim/payloads.csv')\n",
    "dfs = balanced_labels_in_split(df,frac=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of the positive and negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_rate = lambda df: 'sentiment: {negative: %s, positive: %s}' % (\n",
    "    (df.sentiment == 0).sum() / df.size,\n",
    "    (df.sentiment == 1).sum() / df.size\n",
    ")\n",
    "\n",
    "print('dataset -->',sentiment_rate(df),sep='\\n')\n",
    "print('train -->',sentiment_rate(dfs['train']),sep='\\n')\n",
    "print('test -->',sentiment_rate(dfs['test']),sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample of the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['train'].sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Tensorflow input functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = train_input_fs(train = dfs['train'])\n",
    "predict_input_fn =  predict_input_fs(**dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Feature columns***\n",
    "\n",
    "From TF-Hub [feature column](https://github.com/tensorflow/hub/blob/master/docs/api_docs/python/hub/text_embedding_column.md)\n",
    "we use [nnlm-en-dim128 module](https://tfhub.dev/google/nnlm-en-dim128/1). Some important facts are:\n",
    "\n",
    "* The module takes **a batch of sentences in a 1-D tensor of strings** as input.\n",
    "* The module is responsible for **preprocessing of sentences** (e.g. removal of punctuation and splitting on spaces).\n",
    "* The module works with any input (e.g. **nnlm-en-dim128** hashes words not present in vocabulary into ~20.000 buckets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def embedded_text_feature_column_f(module_spec=\"https://tfhub.dev/google/nnlm-en-dim128/1\"):\n",
    "    ''' Call the text embedding from the TF-Hub library\n",
    "    '''\n",
    "    return hub.text_embedding_column(\n",
    "        key=\"payload\", module_spec=module_spec\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the embedded text feature column. If it's called it for the first time, it may take time to download it (it weights around 500 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_text_feature_column = embedded_text_feature_column_f()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Premade Estimators***\n",
    "\n",
    "For classification, we use a premade [DNN Classifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def load_estimator(\n",
    "    embedded_text_feature_column, estimator_f=tf.estimator.DNNClassifier,model_dir = None\n",
    "):\n",
    "    ''' Load the TF `estimator`\n",
    "    '''\n",
    "    return estimator_f(\n",
    "        model_dir = model_dir,\n",
    "        hidden_units=[500, 100],\n",
    "        feature_columns=[embedded_text_feature_column],\n",
    "        n_classes=2,\n",
    "        optimizer=tf.keras.optimizers.Adagrad(lr=0.003)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = load_estimator(embedded_text_feature_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When no *model_dir* is provided, the `estimator` is stored in a temporary file. A deletion of an  `estimator` (either explicit or by the garbage collector) does not erase its corresponding directory from the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = estimator.model_dir\n",
    "del estimator \n",
    "assert path.isdir(model_dir)\n",
    "rmtree(model_dir)\n",
    "estimator = load_estimator(embedded_text_feature_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Evaluate and Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the estimator for a reasonable amount of steps.  \n",
    "Training for 5,000 steps means 640,000 training examples with the default\n",
    "batch size (128).   \n",
    "This is roughly equivalent to 640,000/363 ~ 1700 epochs since the training dataset\n",
    "contains 363 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train(estimator, train_input_fn,steps=5000):\n",
    "    '''Train with TF `estimator.train`\n",
    "    '''\n",
    "    result = {}\n",
    "    for name,input_fn in train_input_fn.items():\n",
    "        result[name] = estimator.train(input_fn=input_fn, steps=steps)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(estimator,train_input_fn,steps=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score is not included in `tf.estimator.evaluate`. The function `f1_score`, calculates the F1 score from the precision and recall. The function `evaluate` adds to  `tf.estimator.evaluate` the  `f1_score` and is acting over the input functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def f1_score(estimations):\n",
    "    ''' Calculates function: \\n\n",
    "    *f1_score(precision,recall) =  (2 x precision x recall) / (precision + recall)*\n",
    "    '''\n",
    "    precision =  estimations['precision']\n",
    "    recall = estimations['recall']\n",
    "    if (precision + recall) < 10**(-12):\n",
    "        return 0.0\n",
    "    \n",
    "    return 2.0 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert f1_score({'precision':10**(-13),'recall':10**(-13)}) == 0.0\n",
    "\n",
    "assert f1_score({'precision':1,'recall':1})  == 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def evaluate(estimator, **args:tf_input_fn):\n",
    "    '''Evaluate with TF `estimator.evaluate`\n",
    "    '''\n",
    "    results = {}\n",
    "    for name,input_fn in args.items():\n",
    "        results[name] = estimator.evaluate(input_fn=input_fn)\n",
    "        results[name]['f1_score'] = f1_score(results[name])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def predict(estimator, df_examples):\n",
    "    ''' Predict with TF `estimator.predict` and from a dataframe of payloads.\n",
    "    '''\n",
    "\n",
    "    def predict_from_input_fn(estimator, **input_fns):\n",
    "        res = {}\n",
    "        for name, input_fn in input_fns.items():\n",
    "            res[name] = np.array(\n",
    "                [x[\"class_ids\"][0] for x in estimator.predict(input_fn=input_fn)]\n",
    "            )\n",
    "        return res\n",
    "\n",
    "    return predict_from_input_fn(\n",
    "        estimator, **predict_input_fs(examples=df_examples)\n",
    "    )['examples']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = evaluate(estimator,**predict_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set accuracy: {accuracy}\".format(**eval_results['train']))\n",
    "print(\"Test set accuracy: {accuracy}\".format(**eval_results['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(estimator, dfs['test'].sample(5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is desirable a predictor that inputs from a string. The function `sentiment` covers this role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def sentiment(estimator, doc):\n",
    "    ''' Gets the sentiment of the *doc* string\n",
    "    '''\n",
    "    ex_df = pd.DataFrame([{'payload':doc,'sentiment':0}])\n",
    "    pred = [\"negative\", \"positive\"][predict(estimator,ex_df)[0]]\n",
    "    return pred #doc + '\\n --> \\n' + pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_str = ''' thank you for offering me the position of Merchandiser with Thomas Ltd.\n",
    "i am thankful to accept this job offer and look ahead to starting my career with your company\n",
    "on June 27, 2000.'''\n",
    "sentiment(estimator,example_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Confusion matrix***  \n",
    "The function `plot_confusion_matrix` plots the corresponding confusion matrix over a dataframe and its corresponding Tensorflow function. The confusion matrix helps  to understand the distribution of misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def plot_confusion_matrix(df_data, estimator, input_fn, header,fig_file=None):\n",
    "    ''' Plot the Confusion Matrix: ${{TN,FN},{FP,TP}}$\n",
    "    '''\n",
    "    def get_predictions(estimator, input_fn):\n",
    "        return [x[\"class_ids\"][0] for x in estimator.predict(input_fn=input_fn)]\n",
    "\n",
    "    LABELS = [\"negative\", \"positive\"]\n",
    "\n",
    "    # Create a confusion matrix on dataframe data.\n",
    "    cm = tf.math.confusion_matrix(\n",
    "        df_data[\"sentiment\"], get_predictions(estimator, input_fn)\n",
    "    )\n",
    "\n",
    "    # Normalize the confusion matrix so that each row sums to 1.\n",
    "    cm = tf.cast(cm, dtype=tf.float32)\n",
    "    cm = cm / tf.math.reduce_sum(cm, axis=1)[:, np.newaxis]\n",
    "\n",
    "    sns.heatmap(cm, annot=True, xticklabels=LABELS, yticklabels=LABELS)\n",
    "    plt.title(header)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    if fig_file is not None:\n",
    "        plt.savefig(fig_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Confusion Matrix for the train set case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(dfs['train'],estimator,predict_input_fn['train'],header='Train data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Confusion Matrix for the test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(dfs['test'],estimator,predict_input_fn['test'],header='Test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load an Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The  `export_estimator` function copy the directory of the estimador given in `estimator.model_dir` to a *dst_model*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def export_estimator(estimator, dst_estimator):\n",
    "    ''' Copy the estimator's directory to a new directory\n",
    "    '''\n",
    "    shutil.copytree(estimator.model_dir, dst_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the saved estimator using `load_model` and check that the evaluation results are up to date with latest check point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TemporaryDirectory() as tmpdirname:\n",
    "    tmp_dir = path.join(tmpdirname, 'tmp')\n",
    "    export_estimator(estimator, dst_estimator=tmp_dir)\n",
    "    loaded_estimator_results = evaluate(\n",
    "        load_estimator(embedded_text_feature_column, model_dir=tmp_dir),\n",
    "        **predict_input_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert loaded_estimator_results == eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erase all the variables defined so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df,dfs, eval_results, embedded_text_feature_column, loaded_estimator_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str('hola this is an string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Pipeline:\n",
    "    '''Implements the workflow: Load -> Train -> Evaluate.'''\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_file=None,\n",
    "        estimator_dir=None,\n",
    "        frac=0.60,\n",
    "        train_steps=5000,\n",
    "        random_state=None\n",
    "    ):\n",
    "        '''Pass the arguments to class attributes.\n",
    "           Load and split the data.\n",
    "        '''\n",
    "        self.frac = frac\n",
    "        self.estimator_dir = estimator_dir\n",
    "        self._is_estimator_dir = path.isdir(str(estimator_dir))\n",
    "        self.random_state = random_state\n",
    "        self.train_steps = train_steps\n",
    "        self.module_spec = \"https://tfhub.dev/google/nnlm-en-dim128/1\"\n",
    "\n",
    "        if src_file is not None:\n",
    "            self.data = pd.read_csv(src_file)\n",
    "            self.split_dataset()\n",
    "\n",
    "    def __del__(self):\n",
    "        ''' Removes the `estimator` and its corresponding directory, \n",
    "        unless the estimator_dir is None.\n",
    "        '''\n",
    "        if not self._is_estimator_dir:\n",
    "            rmtree(self.estimator_dir, ignore_errors=True)\n",
    "\n",
    "    def split_dataset(self):\n",
    "        ''' Train-test splits. Deletes empty dataframes.\n",
    "        '''\n",
    "        self.dfs = balanced_labels_in_split(\n",
    "            self.data, random_state=self.random_state, frac=self.frac\n",
    "        )\n",
    "        if self.dfs['test'].shape[0] == 0:\n",
    "            del self.dfs['test']\n",
    "        if self.dfs['train'].shape[0] == 0:\n",
    "            del self.dfs['train']\n",
    "\n",
    "    def input_fns(self):\n",
    "        self.input = {}\n",
    "        if 'train' in self.dfs.keys():\n",
    "            self.input['train'] = train_input_fs(train=self.dfs['train'])\n",
    "\n",
    "        self.input['predict'] = predict_input_fs(**self.dfs)\n",
    "\n",
    "    def load_estimator(self):\n",
    "        self.embedded_text_feature_column = embedded_text_feature_column_f(\n",
    "            module_spec=self.module_spec\n",
    "        )\n",
    "        self.estimator = load_estimator(\n",
    "            self.embedded_text_feature_column, model_dir=self.estimator_dir\n",
    "        )\n",
    "        self.estimator_dir = self.estimator.model_dir\n",
    "\n",
    "    def train(self):\n",
    "        if self.input['train'] is not None:\n",
    "            train(self.estimator, self.input['train'], steps=self.train_steps)\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.evaluation = evaluate(self.estimator, **self.input['predict'])\n",
    "\n",
    "    def plot_confusion_matrix(self, label, fig_file=None):\n",
    "        plot_confusion_matrix(\n",
    "            self.dfs[label],\n",
    "            self.estimator,\n",
    "            self.input['predict'][label],\n",
    "            header=label + ' data',\n",
    "            fig_file=fig_file\n",
    "        )\n",
    "\n",
    "    def export_estimator(self, dst_dir):\n",
    "        try:\n",
    "            _ = self.estimator\n",
    "        except:\n",
    "            self.load_estimator()\n",
    "        export_estimator(self.estimator, dst_dir)\n",
    "\n",
    "    def predict(self, df_examples):\n",
    "        '''Predict from dataframe'''\n",
    "        return predict(self.estimator, df_examples)\n",
    "\n",
    "    def sentiment(self, doc):\n",
    "        return sentiment(self.estimator, doc)\n",
    "\n",
    "    def pipeline(self):\n",
    "        ''' The pipeline flow is:\n",
    "            input_fns --> load_estimator --> train --> evaluate\n",
    "        '''\n",
    "        self.input_fns()\n",
    "        self.load_estimator()\n",
    "        self.train()\n",
    "        self.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *\n",
    "show_doc(Pipeline.__del__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When an estimator is deleted, the corresponding temporary directory is kept in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline(src_file ='../data/interim/payloads.csv')\n",
    "pl.load_estimator()\n",
    "model_dir  = pl.estimator.model_dir\n",
    "\n",
    "assert path.isdir(model_dir)\n",
    "del pl.estimator\n",
    "assert path.isdir(model_dir)\n",
    "rmtree(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `__del__`  in the `Pipeline` class fixes this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline()\n",
    "pl.load_estimator()\n",
    "model_dir  = pl.estimator.model_dir\n",
    "\n",
    "assert path.isdir(model_dir)\n",
    "del pl\n",
    "assert not path.isdir(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deletion of the `Pipeline` instance does not cause the deletion of the estimator directory when the estimator has been previously loaded (specifying it in *estimator_dir*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TemporaryDirectory() as tmpdirname:\n",
    "    tmp_dir = path.join(tmpdirname, 'tmp')\n",
    "    pl = Pipeline()\n",
    "    pl.load_estimator()\n",
    "    model_dir = pl.estimator.model_dir\n",
    "    pl.export_estimator(dst_dir=tmp_dir)\n",
    "    del pl\n",
    "    \n",
    "    assert tmp_dir != model_dir\n",
    "    assert not path.isdir(model_dir) \n",
    "    assert path.isdir(tmp_dir)\n",
    "    \n",
    "    pl = Pipeline(estimator_dir=tmp_dir)\n",
    "    del pl \n",
    "    \n",
    "    assert path.isdir(tmp_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not generate the train TF input function when train dataframe is not provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = lambda sentiment, n: {'sentiment': [sentiment for _ in range(n)]}\n",
    "df_example_f = lambda sentiment, ni, nf: pd.DataFrame(\n",
    "    sentiments(sentiment, nf - ni), index=range(ni, nf)\n",
    ").sample(nf - ni)\n",
    "\n",
    "pl = Pipeline()\n",
    "pl.load_estimator()\n",
    "pl.dfs = {'test': df_example_f(1,0,100)} #no tes\n",
    "pl.input_fns()\n",
    "assert 'train' not in pl.input.keys()\n",
    "assert 'test' in pl.input['predict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *\n",
    "show_doc(Pipeline.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline` calls the functions that prepares the input data, trains the model and stores the results of the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After running the pipeline, check that all methods are implemented correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(src_file= '../data/interim/payloads.csv')\n",
    "pipeline.pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting evaluation is stored in `pipeline.evaluation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.evaluation.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.evaluation['test']    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix for train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.plot_confusion_matrix('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confustion matrix for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.plot_confusion_matrix('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check save in file option in plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = './this_is_temporary_file_a4gdknlkhlalhfkjdadkl234235253'\n",
    "rmtree(tmp_dir,ignore_errors=True)\n",
    "!mkdir -p {tmp_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_file = path.join(tmp_dir,'fig.png')\n",
    "pipeline.plot_confusion_matrix('test', fig_file=tmp_file)\n",
    "assert path.isfile(tmp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](\"./this_is_temporary_file_a4gdknlkhlalhfkjdadkl234235253/fig.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r {tmp_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TemporaryDirectory() as tmpdirname:\n",
    "    tmp_file = path.join(tmpdirname, 'tmp.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the `export_estimator` is rightly copying the model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TemporaryDirectory() as tmpdirname:\n",
    "    tmp_dir = path.join(tmpdirname,'tmp')\n",
    "    pipeline.export_estimator(tmp_dir)\n",
    "    \n",
    "    assert listdir(tmp_dir) == listdir(pipeline.estimator.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict examples from dataframe in 0 1 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.predict(pipeline.data.sample(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict from string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_str='''thank you very much for offering me the opportunity to work at Norris, Wilson and Kelley as Clinical embryologist.\\nafter much deliberation, i will not be accepting the position,\n",
    "as it isn’t the right fit for my long-term career goals.\\ni sincerely appreciate the offer and give you\n",
    "my best wishes in finding a suitable candidate for the position.\\ni wish you and the company well \n",
    "in all future endeavours.'''\n",
    "print(pipeline.sentiment(doc_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of a `Pipeline` and run it (call `pipeline` method). Export the `estimator`. \n",
    "Create a new instance of a `pipeline` with the exported `estimator`. Evaluate in this pipeline without training.\n",
    "The resulting evaluations of the two pipelines must coincide, provided that the `evaluation` is carried out in the same test set \\[CAVEAT: They must coincide up to a factor (see below)\\]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = randint(0,2**32-1)\n",
    "with TemporaryDirectory() as tmpdirname:\n",
    "    tmp_dir = path.join(tmpdirname, 'tmp')\n",
    "    pl1 = Pipeline(src_file='../data/interim/payloads.csv',random_state=random_state)\n",
    "    pl1.pipeline()\n",
    "    pl1.export_estimator(dst_dir=tmp_dir)\n",
    "    pl2 = Pipeline(src_file='../data/interim/payloads.csv',estimator_dir=tmp_dir,random_state=random_state)\n",
    "    pl2.input_fns()\n",
    "    pl2.load_estimator()\n",
    "    pl2.evaluate()\n",
    "    \n",
    "assert  pl1.evaluation ==  pl2.evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the Pipeline class twice with the same seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-71010aac77f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m'src_file'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'../data/interim/payloads.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m }\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpl2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    'random_state': randint(0, 2**32 - 1),\n",
    "    'src_file': '../data/interim/payloads.csv'\n",
    "}\n",
    "pl1, pl2 = Pipeline(**kwargs), Pipeline(**kwargs)\n",
    "pl1.pipeline()\n",
    "pl2.pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the seed is the same from both pipelines, the train/test split is the same in both cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all((pl1.dfs['train'] ==pl2.dfs['train']).values)\n",
    "assert np.all((pl1.dfs['test'] ==pl2.dfs['test']).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tensorflow training algorithms are not completely [*deterministic*](https://github.com/NVIDIA/tensorflow-determinism). Therefore, the scores in the two pipelines are equal up to a factor. Calculate that factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_equal_pipe_evals(m, n):\n",
    "    ''' $error  = m 10^{-n}$\n",
    "    '''\n",
    "    return all(\n",
    "        abs(\n",
    "            pl1.evaluation[dataname][scorename] -\n",
    "            pl2.evaluation[dataname][scorename]\n",
    "        ) < m * 10**(-n) for dataname in ('train', 'test')\n",
    "        for scorename in pl1.evaluation['train']\n",
    "    )\n",
    "\n",
    "for n,m in product(reversed(range(1, 4)),range(1, 10) ):\n",
    "    if is_equal_pipe_evals(m, n):\n",
    "        break\n",
    "        \n",
    "print(f'The scores with different instances of the pipeline class are equal up to a factor of {m} x 10^({-n})') \n",
    "\n",
    "assert is_equal_pipe_evals(m,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
